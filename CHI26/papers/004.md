*HCI Paper Analysis · TTS Optimized · ~10 min listen*

**Title:** Qualitative Coding Analysis through Open-Source Large Language Models: A User Study and Design Recommendations
**Authors:** Tung T. Ngo, Dai Nguyen Van, Anh-Minh Nguyen, Phuong-Anh Do, Anh Nguyen-Quoc
**Venue:** CHI 2026 — Late-Breaking Work (6 pages)
**One-liner:** An on-device qualitative coding tool using a 20.9B open-source LLM that reveals a trust paradox — users appreciated the speed but didn't trust the depth, even when their data never left the machine.

---

## TL;DR — Why You Should Care

If you care about AI-assisted qualitative research, and you should, this paper tackles a question most tools dodge: what happens when you give researchers an LLM that runs entirely on their own laptop? No cloud. No data leaving the machine. The authors built ChatQDA, a framework that uses a 20.9-billion-parameter open-source model, quantized down to fit in 16 gigs of RAM, to help with qualitative coding tasks like initial coding, focused coding, and code grouping.

They studied eight qualitative researchers, half novice, half experienced, and found something counterintuitive. **Even though the system was provably local, users still didn't fully trust it.** They exhibited what the authors call "conditional trust" — happy to let the AI handle surface-level extraction, but skeptical the moment it touched deeper interpretive work. Privacy ratings landed at a mediocre 3 out of 5, despite the system literally never sending data anywhere. That gap between technical reality and psychological comfort is the real finding here, and it matters for anyone building AI tools for knowledge workers.

---

## The Core Contribution

This paper makes three intertwined contributions. The primary one is an **artifact contribution**: ChatQDA itself, a working on-device framework for qualitative data analysis using open-source LLMs. The secondary contribution is **empirical**: a mixed-methods user study with eight participants across four coding sessions each. And there's a **methodological** thread as well: three design recommendations for building trustworthy AI-assisted QDA tools.

The technical architecture is worth understanding. They use a model called gpt-oss-20b, a 20.9-billion-parameter Mixture-of-Experts model. Think of Mixture-of-Experts like a team of specialists — the model routes each task to the sub-network best equipped to handle it, rather than running everything through a single massive network. They quantized this model using MXFP4 compression down to 12.8 gigabytes, which means it runs on consumer hardware with 16 gigs of RAM. No GPU required. That's a meaningful engineering achievement because it makes local deployment genuinely accessible.

I'd classify the contribution strength as **incremental**. The on-device angle is a worthwhile extension of existing LLM-for-QDA work, and the trust findings are genuinely interesting. But this is a six-page late-breaking work paper, so the study scale is necessarily limited and the system itself builds on established ideas. What elevates it beyond routine is the trust paradox the study uncovered, which points to a deeper design challenge that the field needs to reckon with.

---

## Paper Evaluation — Strengths and Weaknesses

The strongest aspect of this paper is the honesty of the findings. The authors didn't build a system and then cherry-pick positive results. They found that **usefulness ratings were lukewarm, landing around 3.0 to 3.25 on a 5-point scale**. Privacy confidence was middling despite genuine technical protections. The "conditional trust" framing is the most valuable contribution because it moves beyond the binary "users trust AI" or "users don't trust AI" and gives us a more nuanced vocabulary for thinking about human-AI collaboration in analytical work.

The second strength is the design recommendations. Rather than stopping at "users were somewhat skeptical," they offer actionable guidance. Make privacy visible and verifiable. Build trust through traceability and consistency rather than just accuracy. Shift the design goal from "fast" to "methodologically defensible." That last one is particularly sharp — it reframes what success means for these tools.

The main weakness is scale. Eight participants is thin, even for qualitative work. The paper acknowledges this, but it limits what you can infer, especially about the novice-versus-experienced split. With only four people per group, any individual variation swamps the signal. We also don't get much detail on how the model's actual coding quality compared to human judgments, which makes it hard to separate trust issues from performance issues. Were users skeptical because the AI's deeper interpretations were genuinely weak, or because they just didn't trust the concept? The paper doesn't fully disentangle that.

---

## Similar Reading

From the paper's own reference list, these are the most closely related works. Xiao and colleagues' 2023 paper on supporting qualitative analysis with large language models is the most direct comparison, establishing the baseline for LLM-assisted coding that ChatQDA builds on. Gao and team's CollabCoder from 2024 explores collaborative qualitative coding with AI, adding the social dimension that ChatQDA doesn't address. Marathe and Toyama's 2018 work on semi-automated coding is an important ancestor, showing earlier attempts at the same problem with simpler tools. And Draxler and colleagues' 2024 study on AI privacy perceptions provides the theoretical backdrop for understanding why users remain skeptical even when privacy is technically guaranteed.

---

## Seena Labs Relevance

This paper is **directly relevant to Seena's core challenge**. Seena is building AI-powered micro-interviews and synthesis agents that need users to trust both the data collection process and the analytical output. The "conditional trust" finding maps exactly onto Seena's adoption risk: product managers might trust Seena to surface behavioral signals and flag interesting patterns, but balk when Seena tries to synthesize those into higher-level insights or thematic narratives.

The three design recommendations translate almost directly. Seena's evidence traceability system, where every insight links back to source evidence, is already implementing recommendation two — trust through traceability. But recommendation one about making privacy visible is a useful prompt. If Seena processes user session data, how visible is the data pipeline to the product teams relying on the insights? And recommendation three, reframing from "fast" to "defensible," could reshape how Seena positions itself. Instead of "get insights faster," the pitch becomes "get insights you can defend in a product review." That's a stronger value proposition for rigorous PMs.

---

## Empirical Evidence Worth Citing

The numbers here are modest but usable. **Perceived security for on-device AI: median 3 out of 5**, even with fully local processing. That's a powerful data point for anyone arguing that technical privacy guarantees are necessary but insufficient. **Privacy concern ratings: 3 to 4 out of 5**, indicating persistent worry despite local deployment. Usefulness ratings of 3.00 to 3.25 out of 5 suggest cautious acceptance rather than enthusiasm. The model specs are also worth noting for technical contexts: 20.9B parameters, Mixture-of-Experts, MXFP4 quantization to 12.8 GiB, runnable on 16GB consumer hardware. That's a concrete benchmark for what's achievable locally in 2025-2026.

---

## Everything is Designed — Social Media Angle

The dinner party version: **Researchers built an AI that runs entirely on your laptop — your data literally never leaves — and people still didn't trust it.** Not because it was bad. Because they couldn't see what it was doing. The lesson isn't about AI capability. It's about AI legibility. If users can't observe and verify the system's reasoning, no amount of technical privacy guarantees will make them comfortable. Trust isn't a feature you ship. It's an experience you design.

A quotable hook for LinkedIn or Substack: "The hardest part of AI adoption isn't the algorithm. It's closing the gap between what's technically true and what people actually believe. This team proved that even provably private AI gets doubted when users can't see the work." You could frame this as an Everything is Designed piece about the UX of trust — how privacy by design isn't enough without transparency by design. The visual metaphor: it's like having a doctor tell you you're healthy but refusing to show you the test results. You need the receipts.

---

## Industry vs. Theory

This paper bridges both worlds, leaning slightly more toward industry application. The system itself is practical and deployable — consumer hardware, open-source model, real coding workflows. The trust findings are empirically grounded and immediately applicable to product design decisions. At the same time, the "conditional trust" concept has theoretical legs. It challenges the assumption that privacy solves the trust problem and suggests we need a more layered model of how trust operates in AI-assisted analytical work. For Seena's purposes, this is a **"steal the design recommendations, cite the trust findings"** paper. The system architecture is interesting but not directly transferable. The insights about what users actually need to feel confident delegating analytical work to AI — that's gold.
