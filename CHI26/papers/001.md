*HCI Paper Analysis · TTS Optimized · ~12 min listen*

**Title:** Sensing What Surveys Miss: Understanding and Personalizing Proactive LLM Support by User Modeling
**Authors:** Ailin Liu and four co-authors
**Venue:** CHI 2026
**One-liner:** A system that uses skin conductance and mouse movement to detect when survey respondents are struggling, then triggers personalized LLM help at precisely the right moment.

---

## TL;DR — Why You Should Care

Here's the core insight that makes this paper worth your time. Most AI assistance systems wait for you to ask for help, or they blast you with support on a fixed schedule. But the authors of this paper asked a different question: **what if the system could sense that you're struggling before you even realize it yourself?**

They built a system that watches two signals while you're filling out online surveys: your skin's electrical conductance, which is a proxy for stress and cognitive load, and how you're moving your mouse. By combining these signals with personalized machine learning classifiers that adapt to your individual patterns over time, the system can predict when you're genuinely having trouble with a question, versus just thinking carefully.

When the system detects real difficulty, it proactively triggers LLM-generated clarifications and explanations. And here's the kicker: when the timing is right, it works. **Response accuracy went up by twenty-one percent**, and **false negatives, where people get things wrong because they didn't get help, dropped from fifty-one percent down to twenty-three percent.** This matters for anyone building AI systems that need to intervene at the right moment, which is exactly what Seena does with micro-interviews.

---

## The Core Contribution

This paper makes two intertwined contributions. The primary one is an **artifact contribution**: a working adaptive system that fuses physiological sensing with behavioral data, personalized classifiers, and LLM-based assistance into a real-time intervention pipeline. The secondary contribution is **empirical**: a rigorous within-subjects study with thirty-two participants that demonstrates the system's effectiveness.

The system's architecture is worth understanding in some detail. It uses electrodermal activity, or EDA, which measures tiny changes in how well your skin conducts electricity. When you're stressed or cognitively loaded, your sympathetic nervous system activates and your skin conductance spikes. They pair this with mouse movement dynamics, things like velocity, hesitation patterns, and cursor trajectory. Together, these signals feed into a classifier that's been personalized to each individual user.

What's clever about the personalization is their threshold adaptation mechanism. They use a rule-based system with six traversal paths. Think of it like a thermostat that's constantly recalibrating. If the system offers help and the person accepts and gets the answer right, the threshold adjusts downward slightly. If the system doesn't offer help and the person gets the answer wrong, the threshold drops sharply, making it more sensitive. This creates a feedback loop that fine-tunes sensitivity to each individual over the course of a session.

I'd classify the contribution strength as **significant**. It meaningfully advances the state of proactive AI assistance by demonstrating that physiological sensing plus personalized timing can dramatically outperform fixed or random intervention schedules. It's not quite transformative because the sensing hardware requirement, specifically EDA, limits immediate scalability. But the principle it validates is powerful.

---

## Paper Evaluation — Strengths and Weaknesses

The strongest aspect of this paper is the experimental design. The within-subjects setup with three conditions, aligned-adaptive, misaligned-adaptive, and random-adaptive, is elegant because it isolates the timing variable. Many papers in this space just compare "AI help versus no help." By keeping the assistance content constant and only varying when it arrives, the authors make a clean argument that **timing is the critical variable**, not the content of the help itself.

The second strength is the ecological validity of the difficulty spillover framing. This isn't an abstract problem. Anyone who's taken a long, mentally taxing survey knows the feeling of cognitive depletion accumulating across items. The authors ground their system in this well-documented phenomenon and show that properly timed interventions can actually break the cascade before it degrades later responses.

The main weakness is the reliance on electrodermal activity sensing, which requires a wearable sensor on the participant's hand. The paper acknowledges this, but it limits the practical deployment path. Mouse movement alone, which the paper also uses, might be sufficient for many applications and would make the system accessible to anyone with a web browser. A key open question is how much accuracy you'd lose by dropping the EDA signal entirely.

A secondary concern is the sample size and population. Thirty-two participants is respectable for a lab study, but it's worth noting that the personalized classifier approach inherently needs per-user calibration. How well this transfers to truly diverse populations, different age groups, varying levels of tech literacy, and different cultural contexts around help-seeking remains to be explored.

---

## Similar Reading

From the paper's own references, several closely related works stand out. First, Conrad and colleagues' work from 2003 and 2006 on system-initiated clarifications in web surveys, which established the foundation for interactive survey support. Second, the ComPeer system by Liu and colleagues from 2024, which built a conversational agent for proactive peer support and found that **timing accounted for forty percent of variance in intervention acceptance**, a finding that directly motivated this work.

Third, the Just-In-Time Information Retrieval agents concept from Rhodes and Maes in 2000, which pioneered the idea of proactive information delivery based on local context. And fourth, Andolina and colleagues' 2018 work on proactive agents that listen to conversations and retrieve related information, which established the pattern of ambient sensing plus proactive support that this paper builds on.

---

## Seena Labs Relevance

This paper is **directly and deeply relevant to Seena's core architecture**. The central challenge it addresses, when to proactively intervene in a user's task flow, is precisely the problem Seena's detection agents face when deciding the optimal moment to trigger a contextual micro-interview.

The threshold adaptation mechanism with its six traversal paths is immediately applicable to Seena. Right now, Seena's interview trigger logic needs to balance signal quality against user disruption. This paper provides a proven framework for continuously recalibrating that threshold per user. The insight that accepting help and answering correctly means the system was slightly too aggressive, while not receiving help and answering incorrectly means the system was far too conservative, maps directly to Seena's challenge of interview timing.

Seena can't rely on EDA sensing since it operates through standard web sessions. But the mouse movement features the paper extracts, velocity, hesitation, trajectory deviation, are all available to Seena's behavioral analytics layer. The paper validates that behavioral signals alone carry meaningful predictive power for cognitive state, which is encouraging for Seena's sensor-free approach. The key design principle to adopt is the **personalized baseline with adaptive thresholds**, calibrating what "struggling" looks like for each individual user rather than applying population-level defaults.

Finally, the difficulty spillover finding is directly relevant to how Seena sequences micro-interviews. If asking a demanding reflection question depletes cognitive resources for subsequent product interactions, Seena needs to account for this cascade effect in its interview scheduling. Lighter, more contextual prompts may outperform deeper but more taxing questions, especially later in a session.

---

## Empirical Evidence Worth Citing

Several numbers from this study are directly citable in Seena's pitch materials and product memos. Aligned-adaptive assistance improved response accuracy by **twenty-one percent** compared to misaligned timing. False negative rates, cases where respondents needed help but didn't get it and answered incorrectly, dropped from **fifty point nine percent to twenty-two point nine percent** with properly timed interventions.

The system also improved perceived efficiency, dependability, and benevolence, meaning users didn't just perform better, they felt better about the experience. And from the related work they cite, the ComPeer finding that **timing accounted for forty percent of variance in intervention acceptance** is an incredibly powerful statistic for making the case that when you ask matters as much as what you ask.

---

## Everything is Designed — Social Media Angle

Here's the dinner party version: **Your products are interrupting users at the wrong time, and they're paying for it with worse decisions.** This paper proves that AI assistance delivered at the right moment improves outcomes by over twenty percent, while the same help delivered at the wrong moment is basically noise. For product managers, the takeaway is that building helpful features isn't enough. You need to build features that know when to show up.

A quotable hook for Substack or LinkedIn: "We spend millions making AI smarter, but this research shows the real unlock is making AI more patient. A twenty-one percent accuracy improvement came not from better answers, but from better timing." You could frame this as a broader "Everything is Designed" piece about how timing is the invisible design variable that product teams consistently underinvest in.

---

## Industry vs. Theory

This paper bridges both worlds. The theoretical foundation around difficulty spillover and adaptive thresholding is grounded in cognitive science literature. But the artifact itself, a working system with real performance gains, makes it immediately actionable for industry. The EDA requirement keeps the full system in the research domain for now, but the mouse-movement-only variant and the threshold adaptation logic are deployable in production today. For Seena, this is a "steal the architecture, adapt the sensors" paper.
