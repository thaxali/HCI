*HCI Paper Analysis · TTS Optimized · ~14 min listen*

**Title:** Designing Computational Tools for Exploring Causal Relationships in Qualitative Data
**Authors:** Han Meng, Qiuyuan Lyu, Peinuan Qin, Yitian Yang, Renwen Zhang, Wen-Chieh Lin, Yi-Chieh Lee
**Venue:** CHI 2026
**One-liner:** A computational tool that helps qualitative researchers discover causal relationships in their data through a three-level workflow — indicators, concepts, causal networks — revealing that AI scaffolding can overcome analytical inertia while creating tension with established research paradigms.

---

## TL;DR — Why You Should Care

Causal relationships are the holy grail of qualitative research. Every interview transcript, every field note, every observation contains implicit causal claims — "I stopped using the product because the onboarding was confusing" — but extracting and mapping those relationships systematically is brutally time-consuming. Most researchers either do it manually in spreadsheets and sticky notes, or they don't do it at all. This paper introduces QualCausal, a computational tool that scaffolds the entire process from raw text to causal network visualization.

The system works in three levels. First, it extracts indicators — specific phrases from the data that carry causal meaning. Second, users abstract those indicators into higher-level concepts. Third, the system identifies and visualizes causal relationships between concepts as interactive network diagrams. **The team validated this through a rigorous three-phase study: a formative workshop with fifteen researchers, system development informed by their feedback, and a feedback study with another fifteen participants.** The technical evaluation is solid — indicator extraction hit eighty-six percent precision and ninety-four percent recall, and concept creation achieved an inter-annotator agreement kappa of 0.92.

But the most fascinating finding isn't about accuracy. It's about what happens to researchers' analytical practices when you give them AI scaffolding. Some felt liberated — the tool overcame their analytical inertia and helped them see patterns they would have missed. Others felt domesticated — the structured workflow pushed them into a "data labeler" mentality that conflicted with their interpretive traditions. **That tension between computational efficiency and epistemological integrity is the real story here.**

---

## The Core Contribution

The primary contribution is an **artifact**: QualCausal itself, an open-source system built with Vue.js, D3.js, Django, and GPT-4.1 for the LLM components. There's a strong secondary **empirical contribution** from the three-phase study, and a meaningful **theoretical contribution** in the design implications about how computational tools restructure analytical cognition.

The system architecture deserves attention because it embodies a specific theory of how causal analysis should be scaffolded. There are three coordinated views. The Indicator View shows a network visualization of extracted causal indicators at the data level — individual phrases from transcripts linked by causal relationships the LLM has identified. The Concept View abstracts those indicators into higher-level conceptual categories and shows causal relationships between concepts, letting researchers see the forest rather than just the trees. The Node View provides a localized subnetwork focused on a single concept and its immediate causal neighborhood. Plus there's a Details and Concepts Panel for drilling into source text.

The design goals emerged from the formative workshop, where fifteen researchers identified three core challenges: the difficulty of keyword-based coding for causal content, the challenge of distinguishing correlation from causation in qualitative data, and the interpretive complexity of building causal narratives from fragmentary evidence. The system addresses each: automated indicator extraction replaces keyword search, explicit causal relationship modeling forces researchers to specify direction and strength, and multi-level visualization supports moving between evidence and interpretation.

I'd classify the contribution strength as **significant**. While individual components — LLM-based extraction, network visualization, concept mapping — exist separately in prior work, the integrated pipeline from raw qualitative data to causal network is novel. More importantly, the feedback study reveals design tensions that anyone building AI tools for analytical work needs to understand. The open-source release on GitHub makes this a genuinely reusable contribution.

---

## Paper Evaluation — Strengths and Weaknesses

The strongest aspect of this paper is the **depth of the qualitative feedback study**. Fifteen participants worked through a real analysis task using a mental health stigma interview dataset, followed by in-depth interviews about their experience. The richness of the participant quotes reveals genuine analytical tensions that surveys would never capture. When participant P9 says the system "turns interpretation into a classification task" and describes how extracted indicators rather than raw text now drive their analytical attention, that's a profound observation about how tools reshape thinking.

The second strength is the **technical evaluation across two independent datasets**. They tested indicator extraction on both their own MHStigmaInterview-20 corpus and the SemEval 2010 Task 8 benchmark, comparing against co-occurrence networks and causal-cue heuristics. QualCausal outperformed both baselines, achieving eighty point six percent precision and eighty-three percent recall for causal relationship extraction on the interview dataset, with ninety-eight point two percent directionality accuracy. That last number matters — getting the direction of causation right is crucial for qualitative research claims.

The third strength is the honesty about **paradigmatic tensions**. The paper doesn't shy away from the fact that some participants — particularly those trained in interpretivist and post-positivist traditions — fundamentally questioned whether computational causal discovery belongs in qualitative research at all. Participant P9's comment that "no qualitative research dares claim they found causation" and participant F4's observation that "I'm not sure if machines can really teach such deep complex perspectives to AI just through text" are not weaknesses the paper hides but findings it foregrounds. That epistemic honesty elevates the contribution.

The main weakness is the evaluation's scope. **All participants worked with the same pre-selected dataset** in a one-hour session. Real qualitative research involves weeks of immersion in data the researcher collected themselves. The system's value for causal discovery might look very different when researchers have deep contextual knowledge versus encountering unfamiliar data. I'd also flag that the within-sentence limitation — the system only identifies causal relationships expressed within single sentences — misses cross-sentence and discourse-level causation, which the authors acknowledge is a significant boundary. Real causal narratives in qualitative data often span paragraphs.

---

## Similar Reading

From the paper's extensive reference list, the most relevant works are these. Braun and Clarke's foundational work on thematic analysis in psychology provides the methodological context for how qualitative coding traditions inform tool design. Spirtes, Glymour, and Scheines' work on causation, prediction, and search gives the theoretical underpinning for computational approaches to causal discovery. Gao and team's CollabCoder from 2024 is the most direct system comparison, addressing collaborative qualitative coding with AI though without the causal focus. Xiao and colleagues' 2020 exploratory causal analysis approach represents prior work on visual analytics for causal relationships in data. And Wan and team's 2024 survey on bridging causal discovery and large language models maps the broader landscape of LLM-powered causal reasoning that QualCausal inhabits.

---

## Seena Labs Relevance

**This paper is deeply relevant to Seena's synthesis layer.** Seena's pipeline moves from behavioral detection to user interviews to insight synthesis. The missing piece in that pipeline is causal reasoning — not just "users did X" but "users did X because of Y, which led to Z." QualCausal's three-level architecture — indicators, concepts, causal networks — maps almost perfectly to a potential Seena synthesis workflow: behavioral signals become indicators, patterns across users become concepts, and the relationships between patterns become causal hypotheses that product teams can act on.

The coordinated visualization approach is especially instructive. Seena could adopt a similar multi-level view: a detail level showing individual user quotes and behavioral evidence, a pattern level showing abstracted themes across users, and a causal level showing hypothesized cause-effect relationships between patterns. The ability to drill from a high-level causal claim all the way down to the specific user quote that supports it is exactly the evidence traceability that builds trust in AI-generated insights.

But the paradigmatic tensions are the real warning. If fifteen qualitative researchers couldn't agree on whether computational causal discovery is epistemologically valid, product managers receiving Seena's causal claims will have similar trust questions. The paper's recommendation that tools position causal relationships as exploratory hypotheses rather than definitive claims should be a design principle for Seena. The framing matters: "our analysis suggests X may contribute to Y" is defensible in a product review; "X causes Y" is not. QualCausal's concept of preserving interpretive autonomy — letting users modify, reject, and reinterpret AI-generated relationships — should also inform how Seena presents synthesis results.

---

## Empirical Evidence Worth Citing

The technical numbers are strong. **Indicator extraction: precision eighty-six point five percent, recall ninety-four point three percent** on the MHStigmaInterview-20 dataset. **Concept creation: inter-annotator agreement kappa of 0.92**, indicating near-perfect consistency. **Causal relationship extraction: precision eighty point six percent, recall eighty-three percent, directionality accuracy ninety-eight point two percent.** Those directionality numbers are especially important — getting the arrow of causation right matters enormously for research validity.

The comparison numbers contextualize the achievement. Co-occurrence networks achieved only thirty-eight point five percent precision on causal extraction versus QualCausal's eighty point six percent. Causal-cue heuristics managed just twenty-two point six percent precision with ten point eight percent recall. Those baselines represent the current standard tools — NVivo's query features and basic co-occurrence methods — so the improvement is substantial and practical. From the formative study: **fifteen participants identified three core challenges in the two-hour workshop** — keyword coding difficulty, correlation-causation confusion, and interpretive complexity. And the finding that participants experienced the system as a "research assistant" creating a "partnership" rather than a replacement tool is a powerful adoption signal worth citing in any pitch about human-AI analytical collaboration.

---

## Everything is Designed — Social Media Angle

The dinner party version: **Researchers built a tool that helps you find the "why" hiding in qualitative data — not just patterns, but cause-and-effect relationships — and then maps them as interactive networks you can explore.** Think of it as turning interview transcripts into a causal map of human behavior. The twist: when they gave it to researchers, some felt empowered and others felt the tool was changing how they think. One researcher said it felt like having a coding partner. Another said it turned interpretation into classification. Same tool, radically different experiences — because the design of analytical tools doesn't just affect efficiency, it affects epistemology.

A quotable hook: "The most interesting finding in this paper isn't about the technology. It's that giving researchers an AI tool for causal analysis made some of them think differently about their own data — and not always in ways they were comfortable with. One researcher noticed they'd stopped reading the original text and started reading AI-extracted indicators instead. That's not a bug or a feature. It's a design consequence we need to take seriously." You could frame this as an Everything is Designed piece about the epistemology of tools — how every analytical tool embeds assumptions about what counts as evidence, what counts as a valid inference, and what the researcher's role should be. The design question isn't just "does it work" but "what kind of thinking does it produce?"

---

## Industry vs. Theory

This paper straddles both worlds with a slight lean toward theory. The system is fully implemented and open-sourced, which gives it practical weight. But the most lasting contributions are theoretical: the observation that computational tools restructure analytical cognition, the concept of "cognitive scaffolding overcoming analytical inertia," the tension between sequential presentation and holistic processing, and the paradigmatic boundary issues when computational tools enter interpretive traditions. These are design principles, not just system features. For industry, the three-level causal analysis pipeline — indicators to concepts to networks — is a reusable architectural pattern. For theory, the finding that tools can shift researchers from "passive acceptance" to "proactive cognitive exploration" while simultaneously risking turning them into "data labelers" is a genuine contribution to understanding human-AI analytical collaboration. For Seena, this is a **"adopt the visualization architecture, heed the epistemological warnings, cite the technical benchmarks"** paper. The causal network visualization could directly inspire Seena's insight presentation layer, while the paradigmatic tensions should inform how Seena positions its analytical outputs — as hypotheses to explore, not conclusions to accept.
