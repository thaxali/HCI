*HCI Paper Analysis · TTS Optimized · ~13 min listen*

**Title:** Behavioral Indicators of Overreliance During Interaction with Conversational Language Models
**Authors:** Chang Liu, Qinyi Zhou, Xinjie Shen, Xingyu Bruce Liu, Tongshuang Wu, Xiang 'Anthony' Chen
**Venue:** CHI 2026, Barcelona
**One-liner:** A study of 77 participants that identifies five distinct behavioral patterns — visible in mouse clicks, scrolling, and copy-paste actions — that predict whether someone is blindly trusting an LLM's output or critically evaluating it.

---

## TL;DR — Why You Should Care

Here's the problem this paper tackles. When someone uses ChatGPT or Claude for a real task, how do you know whether they're actually thinking critically about what the AI says, or just blindly accepting it? You can check the final output, sure. But by then it's too late. The misinformation is already baked into their work. **What if you could detect overreliance in real time, while it's happening, by watching how someone interacts with the interface?**

That's exactly what these researchers did. They had seventy-seven participants complete three real-world tasks, writing, article summarization, and trip planning, using an LLM that had been deliberately injected with plausible misinformation. Then they logged every click, scroll, copy, paste, and keystroke. By encoding these action sequences with an autoencoder and clustering them with DBSCAN, they identified five behavioral patterns that reliably distinguish careful users from overreliant ones.

The low-overreliance patterns include careful initial task comprehension, reading through the full LLM response before acting, and fine-grained navigation where users scroll to specific words and sentences rather than jumping to rough page regions. **The high-overreliance patterns are revealing: frequent wholesale copy-paste, skipping the initial comprehension step entirely, repeatedly bouncing back to the LLM chat without checking other sources, coarse "ballpark" scrolling and cursor placement, and a particularly fascinating pattern where users hesitate at misinformation but accept it anyway.** That last one is gold for anyone building behavioral intelligence systems.

---

## The Core Contribution

This paper makes a primary **empirical contribution** with a secondary **methodological** contribution. The empirical finding is the five behavioral patterns and their correlation with overreliance levels. The methodological contribution is the analysis pipeline itself: an autoencoder-based approach to encoding variable-length interaction sequences into fixed-size embeddings, followed by DBSCAN clustering to discover recurring behavioral patterns without predefined categories.

The analysis pipeline deserves attention because it's genuinely reusable. They segment interaction logs into overlapping time-based windows at multiple granularities, from ten seconds to sixty seconds. Each window of actions gets encoded as a standardized feature vector. An autoencoder compresses these into compact latent representations. Then DBSCAN, a density-based clustering algorithm that doesn't require you to specify the number of clusters in advance, groups similar behavioral sequences together. They trained separate models for each combination of task and window size, eighteen models total, and looked for patterns that appeared consistently across tasks.

The five patterns they found aren't just statistical artifacts. They make intuitive sense. Users who read the LLM's response carefully before doing anything with it catch more errors. Users who copy entire paragraphs at once, without reading first, absorb the misinformation wholesale. Users who scroll with coarse, "rough landing" movements are essentially skimming, while users who navigate to specific words and phrases are actually engaging with the content. And the hesitation pattern, where users pause at an error, seem to notice something is off, but then accept it anyway, suggests that overreliance isn't always about not noticing problems. Sometimes it's about not having the confidence or motivation to push back.

I'd rate the contribution strength as **significant**. This work meaningfully advances our understanding of how to detect overreliance through process-level behavioral signals rather than outcome-based evaluation. It's not quite transformative because it stays correlational and doesn't yet demonstrate real-time detection or intervention. But it lays the groundwork for systems that could.

---

## Paper Evaluation — Strengths and Weaknesses

The biggest strength is the **shift from outcome-based to process-based measurement of overreliance**. Almost all prior work measures overreliance by comparing task outcomes with and without AI. This paper argues, convincingly, that by the time you check the outcome, you've missed the window for intervention. By identifying behavioral signals during the interaction, the authors open up the possibility of just-in-time detection and mitigation. This is a conceptually important move for the field.

The second strength is the ecological design. Three different tasks, writing, summarization, and trip planning, with task-specific misinformation injection methods, gives the findings much broader applicability than a single-task study. The fact that the behavioral patterns recur across tasks suggests they're tapping into general interaction tendencies rather than task-specific strategies. Seventy-seven participants is also a respectable sample for this kind of detailed behavioral logging study.

The primary weakness is that the relationship between behavioral patterns and overreliance is correlational, not causal. The authors are careful about this, but it means we can't say that coarse scrolling causes overreliance. It could be that users who are less motivated or more time-pressed both scroll coarsely and accept misinformation, with both behaviors stemming from a shared underlying cause. The fifteen-minute time limit on tasks may have amplified this, pushing some participants toward shortcuts they wouldn't normally take.

A secondary concern is the misinformation injection methodology. While well-designed, it creates a somewhat artificial setup. In real-world LLM use, errors aren't uniformly distributed or always "plausible." Sometimes LLMs are wildly wrong in obvious ways, and sometimes they're subtly wrong in ways that even experts miss. How these behavioral patterns hold up across different types and severities of error is an open question the authors acknowledge but can't yet answer.

---

## Similar Reading

The paper's intellectual lineage runs through several key references. First and foremost is the foundational work by Rzeszotarski and Kittur from 2011 on using interaction patterns to predict crowdworker task quality, which established the idea that behavioral traces can serve as quality indicators. Second, Gadiraju and colleagues' 2019 work on understanding crowd workers' behavior and task quality, which extended behavioral analysis to more complex task settings.

On the overreliance side, Buçinca and colleagues' work on cognitive forcing functions to reduce overreliance on AI is a key reference, as it represents one of the few attempts to actually intervene against overreliance rather than just measure it. Vasconcelos and colleagues' 2023 work on explanations and overreliance is also relevant, exploring whether showing users how AI reaches its conclusions helps or hurts. And Passi and Vorvoreanu's 2022 framing of overreliance as "users accepting incorrect LLM recommendations" provides the definitional foundation the paper builds on.

---

## Seena Labs Relevance

**This paper is foundational for Seena's behavioral analytics layer.** The five behavioral patterns identified here map almost directly to the kinds of signals Seena's detection agents should be monitoring during product sessions. But the application is inverted in an interesting way. While this paper detects overreliance on AI, Seena needs to detect behavioral signals that indicate a user is struggling with a product, confused by a feature, or encountering friction that warrants a micro-interview.

The autoencoder-plus-clustering pipeline is immediately adaptable to Seena's behavioral clustering system. Seena already does multi-dimensional session clustering. This paper provides a validated methodology for encoding variable-length interaction sequences into fixed-size embeddings suitable for clustering. The specific approach of overlapping time windows at multiple granularities is a smart technique Seena should adopt. It means you can detect both momentary behavioral shifts, like a ten-second hesitation, and sustained patterns, like sixty seconds of unfocused scrolling, with the same framework.

The **hesitation pattern** is the single most valuable finding for Seena. Users who notice something is wrong but accept it anyway represent a specific behavioral state: they have a question or concern but lack the confidence or context to act on it. This is precisely the moment when a Seena micro-interview should trigger. A short, contextual prompt like "You seemed to pause here — was something unclear?" could convert that hesitation from a silent acceptance of confusion into an actionable insight.

The distinction between fine-grained and coarse-grained navigation patterns also maps to Seena's engagement quality metrics. Fine-grained scrolling, where someone navigates to specific words and sentences, suggests deep engagement. Coarse scrolling suggests surface-level interaction. Seena could use this distinction to weight behavioral signals differently: a micro-interview triggered after deep engagement is more likely to yield substantive responses than one triggered after superficial browsing.

Finally, the copy-paste frequency finding has implications for Seena's interview response quality. If a product user is frequently copying content from AI tools into their workflow, that behavioral signature could indicate they're in a task-completion mindset rather than a reflective one. Seena might want to delay micro-interviews until the user transitions out of rapid task execution and into a more evaluative state, which the behavioral patterns here could help identify.

---

## Empirical Evidence Worth Citing

The headline number is the study scale: **seventy-seven participants across three real-world tasks**, which is substantial for a behavioral logging study of this depth. The five behavioral patterns were identified using eighteen separate autoencoder models, one per combination of three tasks and six time-window sizes, lending robustness to the findings.

The specific behavioral contrasts are citable. Users with low overreliance spend measurably more time on initial task comprehension before interacting with the LLM output. Users with high overreliance show **frequent wholesale copy-paste operations** and **coarse scrolling patterns that target rough page regions rather than specific content**. The hesitation-then-acceptance pattern, where users pause at misinformation but accept it regardless, appeared consistently across tasks and time windows, suggesting it's a robust behavioral signature. The methodological contribution of using DBSCAN clustering on autoencoder embeddings of interaction sequences is itself citable as a validated approach for behavioral pattern discovery.

---

## Everything is Designed — Social Media Angle

The dinner party version: **Researchers found they can tell whether you're actually thinking about what ChatGPT says, or just blindly trusting it, just by watching how you scroll and copy-paste.** Five specific mouse and keyboard patterns predict whether you'll catch AI mistakes or absorb them. The creepiest finding? Some people clearly notice something is wrong, they hesitate, they pause, but they accept the AI's answer anyway. We're not just over-trusting AI. We're over-trusting it even when our instincts are screaming that something is off.

A quotable hook: "The most dangerous moment in AI interaction isn't when you miss the error. It's when you see the error and accept it anyway. That hesitation-then-acceptance pattern tells us overreliance isn't a knowledge problem, it's a confidence problem." You could frame this as an Everything is Designed piece about how the design of AI interfaces, specifically the smooth, confident presentation of LLM output, may be actively suppressing people's healthy skepticism. When the AI speaks in perfectly formatted paragraphs with zero hedging, who are you to question it?

---

## Industry vs. Theory

This paper leans theoretical and empirical, but the path to industry application is clear and short. The behavioral patterns themselves are directly observable in any web application with standard event logging. The autoencoder-clustering pipeline is computationally lightweight enough for production. The missing piece, which the authors acknowledge, is real-time detection and intervention. But the groundwork is laid. For Seena specifically, this is a **"steal the methodology, adapt the targets" paper**. The pipeline is proven. The behavioral indicators are validated. Seena just needs to retrain the model to detect moments of user confusion and frustration rather than AI overreliance, and then trigger micro-interviews at those moments instead of mitigation nudges.
