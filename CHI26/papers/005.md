*HCI Paper Analysis · TTS Optimized · ~14 min listen*

**Title:** Reflexis: Supporting Reflexivity and Rigor in Collaborative Qualitative Analysis through Design for Deliberation
**Authors:** Runlong Ye, Oliver Huang, Patrick Yung Kang Lee, Michael Liut, Carolina Nobre, Ha-Kyung Kong
**Venue:** CHI 2026
**One-liner:** A collaborative workspace that makes qualitative researchers more rigorous by design — using reflexivity prompts, transparent code evolution tracking, and structured disagreement mechanisms to keep analytical teams honest.

---

## TL;DR — Why You Should Care

Qualitative research has a rigor problem, and everyone in the field knows it. When teams of researchers code data together using Reflexive Thematic Analysis, or RTA, the process can easily drift. People unconsciously converge toward groupthink, abandon codes without documenting why, or skip the self-reflection that Braun and Clarke's methodology demands. This paper introduces **Reflexis, a collaborative workspace that doesn't just help you code — it forces you to be honest about how you're coding.**

The system has three clever mechanisms. First, in-situ reflexivity prompts that ask you to examine your own assumptions while you're actively coding, not after the fact. Second, a transparent code evolution history with drift alerts that flag when your coding patterns have shifted over time. Third, a structured disagreement system that makes it productive rather than awkward for team members to challenge each other's interpretations. The team evaluated this with twelve researchers working in pairs, and the results were striking. **All twelve participants felt they remained in control of their analytical decisions**, while eleven out of twelve found the transparency and disagreement features genuinely useful.

If you build tools for qualitative research or care about how AI integrates with analytical workflows, this paper offers a thoughtful framework they call "Design for Deliberation" that goes well beyond the typical "here's a coding tool with GPT bolted on" approach.

---

## The Core Contribution

The primary contribution here is an **artifact**: Reflexis itself, a working collaborative platform for Reflexive Thematic Analysis. But there's a strong secondary **theoretical contribution** in the "Design for Deliberation" framework, and a solid **empirical contribution** from the two-phase evaluation.

Let me walk through the three core mechanisms because they're each solving a distinct problem. The first is ReflexiveLens, which generates in-situ reflexivity prompts. When you're coding a passage, the system uses an LLM to surface questions about your positionality and assumptions. It's like having a thoughtful colleague who occasionally asks "why did you interpret it that way?" while you're in the middle of working. The key design choice: these prompts appear in context, during the act of coding, rather than in a separate reflection journal after the fact. That matters because reflexivity that happens in the moment shapes the actual analysis, whereas post-hoc reflection is just retrospective rationalization.

The second mechanism is the analysis history with Code Drift Alert. This tracks how your codes evolve over time and flags when your coding patterns have shifted significantly. Think of it like version control for your analytical thinking. If you coded the first ten interviews one way and gradually drifted in interviews fifteen through twenty, the system catches that and asks you to acknowledge and explain the shift. This addresses a known problem in qualitative research where coders unconsciously evolve their interpretive frames without documenting the change.

The third mechanism is Discussion Focus combined with positionality-aware collaboration prompts. When two researchers disagree about how to code a passage, the system doesn't just flag the disagreement — it structures the conversation around it. It reminds each researcher of their stated positionality and asks them to engage with the specific basis of their disagreement. This transforms conflict from something awkward into something analytically productive.

The contribution strength is **significant**. It meaningfully advances how we think about tool support for qualitative rigor. Rather than automating coding, which is what most AI-QDA tools try to do, Reflexis automates the quality assurance around coding — the meta-analytical practices that separate rigorous research from casual pattern-matching. That's a genuinely different design philosophy.

---

## Paper Evaluation — Strengths and Weaknesses

The strongest aspect of this paper is the **Design for Deliberation framework**. Rather than just presenting a tool and some user study results, the authors articulate a principled design philosophy that could guide future work across the entire space of AI-assisted analytical tools. The framework has three pillars: enabling reflection, supporting transparency, and facilitating principled disagreement. Each pillar maps to a specific system mechanism, and each mechanism is grounded in formative research. That alignment between theory, design rationale, and implementation is rare and well-executed.

The second strength is the two-phase evaluation approach. Phase one was a formative study with fifty-five survey respondents and three in-depth interviews, which shaped the design. Phase two was a controlled evaluation with twelve participants working in six pairs through 1.5 to 2-hour sessions. The combination gives both breadth of insight about what researchers need and depth of evidence about whether Reflexis delivers. The quantitative findings are encouraging — ten out of twelve agreed the prompts encouraged deeper reflection, eleven out of twelve found the analysis history improved transparency, eleven out of twelve found Discussion Focus useful for navigating disagreements.

The technical implementation is also worth noting. Built with Next.js, Tailwind CSS, Firebase, and React Flow, using GPT-5 and GPT-5-mini for the reflexivity prompts, the system is modern and open-sourced on GitHub. That's a meaningful contribution to the research community.

The main weakness is the evaluation's ecological validity. Pairs of researchers worked for ninety minutes to two hours with pre-selected datasets. Real RTA projects unfold over weeks or months with researchers developing deep familiarity with their data. The Code Drift Alert mechanism, which flags shifts in coding patterns over time, is hard to properly evaluate in a single session. The authors acknowledge this, but it means the most novel feature is also the least validated. I'd also have liked to see more analysis of the Sankey diagrams showing reflexivity shifts — the paper mentions a shift from low to high reflexivity frequency but doesn't deeply unpack what that transition looked like qualitatively.

---

## Similar Reading

From the paper's reference list, the most relevant related works are these. Braun and Clarke's foundational work on Reflexive Thematic Analysis provides the methodological backbone that Reflexis is built to support — you need to understand RTA to appreciate what the tool is doing. Jiang and colleagues' 2021 work on supporting serendipity in qualitative coding explores a related idea of how tools can surface unexpected connections during analysis. Gao and team's CollabCoder from 2024 is the closest system comparison, tackling collaborative qualitative coding with AI but without the reflexivity focus. McDonald and colleagues' 2019 paper on reliability in qualitative analysis addresses the fundamental question of how we assess rigor in interpretive work. And Deterding and Waters' 2021 piece on flexible coding offers an alternative perspective on what structured qualitative analysis could look like.

---

## Seena Labs Relevance

This paper is **highly relevant to Seena on multiple levels**. The Design for Deliberation framework directly informs how Seena should think about its synthesis agents. When Seena's AI synthesizes behavioral data into insights, there's a version of the reflexivity problem at play: are the AI's interpretive frames drifting? Are early patterns biasing later analysis? The Code Drift Alert concept could be adapted for Seena's multi-agent pipeline — monitoring whether synthesis agents maintain consistent interpretive standards across a growing dataset.

The traceability mechanisms validate Seena's existing evidence-traceability architecture, where every insight links back to source evidence. Reflexis goes further by also tracking the evolution of codes over time, which is a feature Seena could adopt. Imagine being able to show a PM not just "here's the insight and here's the supporting evidence" but also "here's how our interpretation of this behavioral pattern evolved as we collected more sessions." That's a powerful credibility signal.

The positionality-aware disagreement system is also provocative. In Seena's multi-agent architecture, what if detection agents and synthesis agents had explicit "positionality statements" — declared biases or analytical tendencies — that were visible to users? That would make the AI's analytical stance transparent rather than opaque, directly addressing the trust gap that ChatQDA identified.

---

## Empirical Evidence Worth Citing

The formative study numbers are useful for framing the problem space. **Of fifty-five surveyed qualitative researchers, the data revealed significant challenges with maintaining reflexivity during active coding** — that's a market validation data point. From the evaluation: **ten out of twelve participants agreed that reflexivity prompts encouraged deeper reflection on their analytical process**. **Eleven out of twelve found the analysis history feature improved transparency in code evolution**. **Eleven out of twelve found Discussion Focus useful for navigating interpretive disagreements**. And perhaps most importantly, **twelve out of twelve reported feeling "in control of analytic decisions"** despite extensive AI involvement. That last number is critical for anyone building AI tools for knowledge workers — it shows that AI augmentation doesn't have to mean loss of agency.

---

## Everything is Designed — Social Media Angle

The dinner party version: **Researchers built a tool that makes qualitative analysis teams more honest — not by watching what they do, but by asking them the right questions at the right time.** It's like having a built-in peer reviewer who notices when your thinking has shifted and asks you to own it. The insight for a general audience: most AI tools try to do the work for you. This one tries to make you better at the work. That's a fundamentally different design philosophy, and it might be the right model for AI in any field where judgment matters more than speed.

A quotable hook: "The most dangerous moment in collaborative analysis isn't when your team disagrees. It's when everyone quietly converges without examining why. This tool catches that silent drift." You could frame this as an Everything is Designed piece about the difference between AI-as-replacement and AI-as-mirror — tools that reflect your own analytical process back at you so you can see your blind spots. The design principle is powerful: sometimes the best intervention isn't doing the work for someone, it's helping them see their own work more clearly.

---

## Industry vs. Theory

This paper straddles the line beautifully. The Design for Deliberation framework is a genuine theoretical contribution that can inform tool design well beyond qualitative research. But Reflexis itself is a fully implemented, open-source system that research teams could adopt tomorrow. The evaluation, while limited in ecological validity, demonstrates practical utility. For Seena, this is a **"adopt the framework, study the mechanisms, consider the architecture patterns"** paper. The theoretical framing is useful for positioning Seena's value proposition around rigor rather than speed. The specific mechanisms — reflexivity prompts, drift detection, structured disagreement — are all directly adaptable. And the open-source codebase means you can look at exactly how they implemented these ideas rather than just reading about them.
