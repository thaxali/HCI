*HCI Paper Analysis · TTS Optimized · ~14 min listen*

**Title:** InterFlow: Designing Unobtrusive AI to Empower Interviewers in Semi-Structured Interviews
**Authors:** Yi Wen and five co-authors
**Venue:** CHI 2026, Barcelona
**One-liner:** An AI-powered visual scaffold that dynamically adapts interview scripts in real time, tracks conversational balance, and surfaces follow-up suggestions through a co-interview agent — all without stealing the interviewer's attention.

---

## TL;DR — Why You Should Care

If you've ever conducted a semi-structured interview, you know the juggling act. You're trying to actively listen to your participant, mentally tracking which questions you've covered and which you haven't, deciding whether to probe deeper or move on, keeping an eye on the clock, and somehow taking notes that you'll actually be able to use later. **InterFlow is a system designed to take several of those balls out of the air for you.**

The authors built an AI-powered interface with three main components. First, an interactive script that transforms your static interview guide into a living visualization, color-coded by what you've covered, what's current, and what's still ahead, with drag-and-drop reordering. Second, a visual timer that shows not just elapsed time but your speaking ratio, so you can see at a glance whether you're talking too much or giving the participant enough space. And third, a mixed-initiative information capture system with three escalating levels of AI involvement: manual notes, AI-generated summaries you can trigger on demand, and a proactive co-interview agent that listens along and surfaces potential follow-up points you might have missed.

They evaluated this against a baseline of a text editor plus OpenAI's realtime speech API in a within-subjects study with twelve participants. The results show InterFlow **significantly enhanced situational awareness**, with the score jumping from 2.67 to 5.0 on a seven-point scale. But here's what makes this paper especially interesting: they also found real limitations in how actionable the AI's suggestions were during the fast-moving reality of a live conversation. That tension between useful AI and usable AI in real-time contexts is exactly the design problem Seena faces.

---

## The Core Contribution

This paper's primary contribution is an **artifact**: a working system that re-imagines the semi-structured interview tool as a dynamic, AI-augmented workspace. The secondary contribution is **empirical**, grounded in a comparative user study, and there's a tertiary **methodological** contribution in their design implications for unobtrusive AI assistance under time pressure.

The system architecture is worth unpacking. The interactive script component uses spaCy for text processing and embeddings to perform question retrieval. As the conversation unfolds, it automatically detects which question the interviewer is currently on and updates the visual state. Questions are color-coded: yellow for the current question, gray for visited, blue for unvisited. Interviewers can drag and drop to reorder the remaining questions on the fly, which is a small but brilliant design choice. It means the AI doesn't dictate the interview flow; it gives you the scaffolding to manage it yourself.

The visual timer is more than a countdown. It embeds a speaking-ratio visualization that shows the balance between interviewer and interviewee talk time. This creates what the authors call a scaffold for metacognition, helping interviewers notice when they're dominating the conversation or when a participant is being unusually brief. Several participants reported using it as a checkpoint during natural pauses to decide whether to wrap up a topic or dig deeper.

The co-interview agent is the most ambitious component. It listens to the conversation continuously, combines conversation analysis with an LLM acting as a judge, and proactively surfaces points worth probing. It explains why it's flagging something, whether that's an inconsistency it detected, a topic the participant mentioned but didn't elaborate on, or a connection to the research questions. The key design decision here is that the agent provides suggestions but never takes action. The interviewer always decides whether and how to follow up.

I'd rate the contribution strength as **significant**. The system meaningfully advances how we think about AI augmentation in qualitative research methods. It's not transformative because the core components, real-time transcription, LLM summarization, proactive suggestions, are individually known. But the integration into a coherent interview scaffold, combined with the honest evaluation of where it falls short, pushes the field forward in a genuinely useful way.

---

## Paper Evaluation — Strengths and Weaknesses

The strongest aspect of this paper is the **design philosophy of graduated automation**. The three levels of information capture, manual notes, on-demand AI summary, and proactive co-interview agent, let users engage with AI at whatever level of involvement they're comfortable with in the moment. This is a textbook example of mixed-initiative interaction done well. It respects the interviewer's agency while offering genuine support. Too many AI tools in this space go all-or-nothing: either the AI runs the show or it sits passive until explicitly invoked. InterFlow finds the middle ground.

The second strength is the intellectual honesty of the evaluation. The authors don't just report that InterFlow reduced cognitive load, which it did. They also carefully document where the system fell short. One participant's quote captures it perfectly: the system flagged an inconsistency, but the interviewer didn't know how to naturally shift the conversation to address it without breaking flow. This gap between recognizing an opportunity and being able to act on it in real time is a critical insight that most papers in this space would gloss over. Participants asked an average of **24.58 questions with InterFlow versus 18.17 with the baseline**, suggesting the system encouraged broader coverage, but the authors are careful to note that more questions doesn't automatically mean better interviews.

The primary weakness is the small sample size. Twelve participants is enough for a formative evaluation and to identify themes, but it limits the statistical power of the quantitative comparisons. The NASA Task Load Index and System Usability Scale results are suggestive but not conclusive at this scale. A larger study with more diverse interviewers, including novices versus experts, would strengthen the claims substantially.

A secondary concern is that the study used staged interviews rather than real research interviews with genuine stakes. The authors acknowledge this, but it matters. In a real study, the interviewer has deeper domain knowledge and stronger opinions about what matters. The co-interview agent's suggestions might be more or less useful when the interviewer already has strong hypotheses. Whether InterFlow helps most when you know your domain well or when you're exploring unfamiliar territory is an open question.

---

## Similar Reading

From the paper's own references, several key works stand out. First, Schroeder and colleagues' 2025 survey of LLM uses in qualitative research, which maps the broad landscape from data collection through analysis. Second, the Interview AI-ssistant work on real-time human-AI collaboration in interview preparation and execution, which tackles the same problem space from a different angle. Third, the work by Jiang and colleagues from 2021 on supporting qualitative research through computational methods, which established the foundation for AI-augmented qualitative workflows.

Also worth noting is the Envisioning AI Support paper that explored how interviewers across the expertise spectrum imagine AI assistance in semi-structured interviews, which provides the formative research that motivates InterFlow's design choices. And Chen and colleagues' 2025 work on AI support in online meetings, which tackles similar challenges of real-time AI assistance during attention-intensive conversational tasks, helping ground the design implications in a broader context.

---

## Seena Labs Relevance

**This paper is the closest thing to a direct blueprint for Seena's micro-interview system that exists in the CHI literature.** While InterFlow targets human interviewers conducting research interviews, nearly every design decision maps to challenges Seena faces with its automated micro-interview agents.

The graduated automation model is immediately applicable. Seena's micro-interviews currently need to decide how much to automate versus how much to let the user direct. InterFlow's three-tier approach suggests Seena should offer analogous levels: let users flag things manually through direct feedback, provide AI-summarized context on demand, and have proactive agents surface follow-up opportunities. The key insight is that **different users will engage with different levels at different moments**, and the system should support all three simultaneously.

The speaking-ratio visualization is directly transferable to Seena's interview analytics. In micro-interviews, the equivalent metric is the balance between system prompts and user responses. If Seena's interview agent is asking too many questions relative to the depth of user responses, that's a signal the questions are too broad or the timing is wrong. Building this kind of conversational balance metric into Seena's interview quality scoring would be straightforward and valuable.

The most important finding for Seena is the actionability gap. InterFlow showed that even when AI suggestions are accurate and well-timed, human interviewers often can't act on them because the conversation has moved on. For Seena, this has a counterintuitive implication: **because Seena's interview agent is the one asking the questions, it can actually act on its own follow-up suggestions in ways human interviewers cannot.** This is a structural advantage of automated micro-interviews over human-conducted interviews augmented by AI. Seena should lean into this advantage by designing follow-up logic that's more aggressive than what a human interviewer could manage.

Finally, the interactive script concept maps to how Seena should manage interview guides. Rather than a fixed question sequence, Seena's interview agents should maintain a dynamic, reorderable script that adapts based on what the user has already revealed through their behavior. The drag-and-drop metaphor from InterFlow could become a configuration interface for product managers setting up Seena micro-interview campaigns, letting them prioritize questions but trusting the system to sequence them optimally for each session.

---

## Empirical Evidence Worth Citing

The headline number is the situational awareness improvement: **from 2.67 to 5.0 on a seven-point scale**, a statistically significant jump with a large effect size of d equals negative 1.07, and p equals .009. The visual timer specifically was rated positively at **5.67 out of 7**, making it the highest-rated individual component.

On the interaction side, interviewers asked **24.58 questions on average with InterFlow versus 18.17 with the baseline**, a thirty-five percent increase in question coverage. The system uses NASA Task Load Index for cognitive load measurement, a well-validated instrument, and the System Usability Scale for usability, both standard references that lend credibility to the evaluation. The qualitative finding that suggestion actionability is constrained by conversational dynamics is also citable as evidence for why fully automated interview agents may outperform AI-augmented human interviewers in certain contexts.

---

## Everything is Designed — Social Media Angle

The dinner party version: **We've been thinking about AI for interviews all wrong. Instead of replacing the interviewer, the real opportunity is becoming their co-pilot.** InterFlow shows that when AI handles the logistics, tracking time, visualizing progress, catching what you missed, the human interviewer is freed up to do what humans do best: listen deeply and follow their instincts. But here's the twist. Even with a great co-pilot, human interviewers couldn't always act on the AI's suggestions fast enough. Which raises an uncomfortable question: in some contexts, might a well-designed AI interviewer actually be better than a human with AI assistance?

A quotable hook for content: "The best AI assistant isn't the one that gives you the most information. It's the one that gives you the right information at the moment you can actually use it. InterFlow proves that timing and actionability matter more than intelligence." You could frame this as an Everything is Designed piece on the design of interruption, when AI should speak up versus shut up, and what that means for how we build products that assist without overwhelming.

---

## Industry vs. Theory

This paper sits firmly in the bridge between theory and practice. The theoretical contributions around unobtrusive AI design and mixed-initiative interaction under time pressure are well-grounded in HCI frameworks. But the system itself is immediately practical. Any UX research team doing regular interviews could benefit from a tool like InterFlow tomorrow. For Seena, this is less of a "steal the architecture" paper and more of a **"steal the design philosophy" paper**. The graduated automation model, the conversational balance metrics, and especially the honest reckoning with the actionability gap are all design principles Seena should internalize as it builds out its micro-interview system.
